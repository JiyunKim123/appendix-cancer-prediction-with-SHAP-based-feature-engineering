import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
import lightgbm as lgb
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score
import shap

# 파일 경로
file_path = "/content/drive/My Drive/Colab Notebooks/appendix_cancer dataset/appendix_cancer_prediction_dataset.csv"

# 데이터 로드
df = pd.read_csv(file_path)

# 불필요한 열 제거
df.drop(columns=['Patient_ID'], inplace=True)

# 레이블 인코딩 (범주형 변수 변환)
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le  # 나중에 해석을 위해 저장

# 특성과 타겟 분리
X = df.drop(columns=['Appendix_Cancer_Prediction'])
y = df['Appendix_Cancer_Prediction']

# 데이터 불균형 해소 (SMOTE 적용)
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# 학습/테스트 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)

# 최적 max_depth 찾기
best_depth = -1
best_score = 0
for depth in [3, 5, 7, 10, 15, 20, -1]:
    temp_model = lgb.LGBMClassifier(objective='binary', metric='binary_error', boosting_type='gbdt', max_depth=depth, seed=42)
    scores = cross_val_score(temp_model, X_train, y_train, cv=3, scoring='accuracy')
    mean_score = scores.mean()
    print(f"max_depth={depth}, Accuracy={mean_score:.4f}")
    if mean_score > best_score:
        best_score = mean_score
        best_depth = depth

print(f"\nOptimal max_depth: {best_depth}")

# 최적 max_depth를 반영하여 모델 학습
best_model = lgb.LGBMClassifier(objective='binary', metric='binary_error', boosting_type='gbdt', max_depth=best_depth, seed=42)
best_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], callbacks=[lgb.early_stopping(10), lgb.log_evaluation(10)])

# SHAP 분석 추가
explainer = shap.Explainer(best_model, X_train)
shap_values = explainer(X_test)

# SHAP Summary Plot
shap.summary_plot(shap_values, X_test)

# SHAP 값을 기반으로 상위 15개 feature 선택
shap_importance = np.abs(shap_values.values).mean(axis=0)
top_15_features = X_train.columns[np.argsort(shap_importance)[-15:]]
print("\nTop 15 Features Based on SHAP:", top_15_features.tolist())

# 상위 15개 feature만 선택하여 데이터 재구성
X_train_top15 = X_train[top_15_features]
X_test_top15 = X_test[top_15_features]

# 최적 max_depth로 다시 모델 학습 (상위 15개 feature만 사용)
best_model.fit(X_train_top15, y_train, eval_set=[(X_test_top15, y_test)], callbacks=[lgb.early_stopping(10), lgb.log_evaluation(10)])

# 예측
y_pred_prob = best_model.predict_proba(X_test_top15)[:, 1]
y_pred = (y_pred_prob > 0.5).astype(int)

# 평가 지표 계산
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# 평가 결과 출력
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
